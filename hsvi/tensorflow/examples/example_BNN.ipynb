{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import numpy as np\n",
    "from hsvi.tensorflow import Hierarchy_SVI\n",
    "from utils.distributions import Normal, OneHotCategorical\n",
    "from utils.train_util import get_next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of training a Bayesian MLP model by variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bayesian_MLP:\n",
    "    def __init__(self,x,net_shape,learning_rate=0.001,num_samples=1,ac_fn=tf.nn.relu):\n",
    "        self.x = x\n",
    "        self.net_shape = net_shape\n",
    "        self.num_samples = num_samples # number of samples of parameters\n",
    "        self.ac_fn = ac_fn\n",
    "        self._build_net()\n",
    "        self._conf_opt(learning_rate)\n",
    "        \n",
    "    def _build_net(self):\n",
    "        self.H,self.W,self.B,self.parm_var = [], [],[],{}\n",
    "        \n",
    "        ### expand input dimensions ###\n",
    "        h = tf.expand_dims(self.x,axis=0)\n",
    "        h = tf.tile(h,[self.num_samples,1,1])\n",
    "        \n",
    "        ### define variables ###\n",
    "        with tf.variable_scope('global'):\n",
    "            for i in range(len(self.net_shape)-1):\n",
    "                ### define variational parameters ###\n",
    "                print('conf layer {}'.format(i))\n",
    "                d1 = self.net_shape[i]\n",
    "                d2 = self.net_shape[i+1]\n",
    "                w_loc_var = tf.get_variable(dtype=tf.float32, initializer=tf.random_normal([d1,d2],stddev=0.001),name='l'+str(i)+'_w_loc')\n",
    "                w_s_var = tf.get_variable(dtype=tf.float32, initializer=tf.ones([d1,d2])*-3.,name='l'+str(i)+'_w_scale')\n",
    "                b_loc_var = tf.get_variable(dtype=tf.float32, initializer=tf.random_normal([d2],stddev=0.001),name='l'+str(i)+'_b_loc')\n",
    "                b_s_var = tf.get_variable(dtype=tf.float32, initializer=tf.ones([d2])*-3.,name='l'+str(i)+'_b_scale')\n",
    "                w = Normal(loc=w_loc_var,scale=tf.exp(w_s_var))\n",
    "                b = Normal(loc=b_loc_var,scale=tf.exp(b_s_var))\n",
    "                self.W.append(w)\n",
    "                self.B.append(b)\n",
    "                self.parm_var[w] = [w_loc_var,w_s_var]\n",
    "                self.parm_var[b] = [b_loc_var,b_s_var]\n",
    "\n",
    "                ### sample parameters to compute output ###\n",
    "                ew = w.sample(self.num_samples)\n",
    "                eb = b.sample(self.num_samples)\n",
    "                z = tf.einsum('sbi,sij->sbj',h,ew)+tf.expand_dims(eb,1)\n",
    "                if i != len(self.net_shape) - 2:\n",
    "                    h = self.ac_fn(z)\n",
    "                else:\n",
    "                    h = OneHotCategorical(logits=z)\n",
    "\n",
    "                self.H.append(h)\n",
    "            \n",
    "    def _conf_opt(self,learning_rate):\n",
    "        ### config optimizer ###\n",
    "        with tf.variable_scope('global'):\n",
    "            step = tf.Variable(0, trainable=False, name='global_step')                                      \n",
    "            self.optimizer = (tf.train.AdamOptimizer(learning_rate,beta1=0.9),step)\n",
    "      \n",
    "    def forward(self,x,sess):\n",
    "        h = x\n",
    "        for l in range(len(self.W)):\n",
    "            w = sess.run(self.W[l].loc)\n",
    "            b = sess.run(self.B[l].loc)\n",
    "            h = tf.add(tf.matmul(h,w),b)\n",
    "            if l != len(self.W)-1:\n",
    "                h = self.ac_fn(h)\n",
    "            else:\n",
    "                h = OneHotCategorical(logits=h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_inference(model,Y,TRAIN_SIZE,vi_type='KLqp',scale=1.):\n",
    "    \n",
    "    ### config priors for parameters ###\n",
    "    prior_cfg = {}\n",
    "    for qw in model.W+model.B:\n",
    "        pw = Normal(loc=tf.zeros_like(qw),scale=tf.ones_like(qw))\n",
    "        prior_cfg[pw] = qw        \n",
    "        \n",
    "    ### config variational inference ###\n",
    "    inference = Hierarchy_SVI(latent_vars={'global':prior_cfg},data={'global':{model.H[-1]:Y}},vi_types={'global':vi_type},scale={model.H[-1]:scale},optimizer={'global':model.optimizer},train_size=TRAIN_SIZE)        \n",
    "\n",
    "    return inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 50000\n",
    "test_size = 10000\n",
    "batch_size = 256\n",
    "epoch = 50\n",
    "hidden = [100,100]\n",
    "num_samples = 1 # number of samples of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/yu/gits/data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /home/yu/gits/data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/yu/gits/data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/yu/gits/data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "### load data ###\n",
    "DATA_DIR = '/home/yu/gits/data/mnist/'\n",
    "data = input_data.read_data_sets(DATA_DIR,one_hot=True)\n",
    "X_TRAIN = data.train.images[:train_size]\n",
    "Y_TRAIN = data.train.labels[:train_size]\n",
    "X_TEST = data.test.images[:test_size]\n",
    "Y_TEST = data.test.labels[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config net shape ###\n",
    "in_dim = X_TRAIN.shape[1]\n",
    "out_dim = Y_TRAIN.shape[1]\n",
    "net_shape = [in_dim]+hidden+[out_dim]\n",
    "\n",
    "### config data input ###\n",
    "x_ph = tf.placeholder(dtype=tf.float32,shape=[None,in_dim])\n",
    "y_ph = tf.placeholder(dtype=tf.float32,shape=[num_samples,None,out_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf layer 0\n",
      "conf layer 1\n",
      "conf layer 2\n",
      "start init hsvi\n",
      "global KLqp\n",
      "config optimizer in scope global\n"
     ]
    }
   ],
   "source": [
    "### define model ###\n",
    "model = Bayesian_MLP(x=x_ph,net_shape=net_shape,num_samples=num_samples)\n",
    "inference = config_inference(model,y_ph,train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 loss 2.6054372787475586\n",
      "epoch 20 loss 1.7771762609481812\n",
      "epoch 30 loss 1.3696476221084595\n",
      "epoch 40 loss 1.1777886152267456\n",
      "epoch 50 loss 1.039338231086731\n"
     ]
    }
   ],
   "source": [
    "### train process ###\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "tf.global_variables_initializer().run(session=sess)\n",
    "for e in range(epoch):\n",
    "    shuffle_inds = np.arange(X_TRAIN.shape[0])\n",
    "    np.random.shuffle(shuffle_inds)\n",
    "    x_train = X_TRAIN[shuffle_inds]\n",
    "    y_train = Y_TRAIN[shuffle_inds]\n",
    "    ii = 0\n",
    "    num_iter = int(np.ceil(x_train.shape[0]/batch_size))\n",
    "    for _ in range(num_iter):\n",
    "        x_batch,y_batch,ii = get_next_batch(x_train,batch_size,ii,labels=y_train)\n",
    "        y_batch = np.expand_dims(y_batch,axis=0)\n",
    "        y_batch = np.repeat(y_batch,num_samples,axis=0)\n",
    "\n",
    "        feed_dict = {x_ph:x_batch,y_ph:y_batch}\n",
    "        info_dict = inference.update(scope='global',feed_dict=feed_dict,sess=sess)\n",
    "    if (e+1)%10==0:\n",
    "        print('epoch {} loss {}'.format(e+1, info_dict['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.9728\n"
     ]
    }
   ],
   "source": [
    "### test process ###\n",
    "ty = model.forward(x_ph,sess)\n",
    "y_pred_prob = sess.run(ty,feed_dict={x_ph:X_TEST})\n",
    "y_pred = np.argmax(y_pred_prob,axis=1)\n",
    "correct = np.sum(np.argmax(Y_TEST,axis=1)==y_pred)\n",
    "acc = correct/Y_TEST.shape[0]\n",
    "print('accuracy is {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36env",
   "language": "python",
   "name": "p36env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
